# Pr√©parer ton environnement complet pour relancer tri_state_ai proprement dans Jupyter Notebook

‚úÖ √âtape 1 ‚Äî Installer Python 3 et v√©rifier son fonctionnement

 T√©l√©chargement (Windows x64)
 Python : https://www.python.org/downloads/windows/
 Important : Cocher ‚ÄúAdd Python to PATH‚Äù pendant l'installation
 Puis tester dans CMD :
python --version

R√©sultat attendu ‚Üí Python 3.x.x

‚úîÔ∏è Python op√©rationnel

################################################

‚úÖ √âtape 2 ‚Äî Installer Jupyter Notebook et v√©rifier

Toujours dans CMD :
python -m pip install --upgrade pip
pip install notebook
Tester :
jupyter --version


‚úîÔ∏è Jupyter est pr√™t

################################################

‚úÖ √âtape 3 ‚Äî Cr√©er & activer l‚Äôenvironnement virtuel

Dans le dossier natif "document", cr√©er data_credit_scoring\tri_state_ai
Dans CMD (Drapeau + R) ‚Üí aller au dossier du projet :
cd C:\Users\33669\OneDrive\–î–æ–∫—É–º–µ–Ω—Ç—ã\data_credit_scoring\tri_state_ai

Cr√©er l‚Äôenvironnement virtuel :
python -m venv .venv

Activer l‚Äôenvironnement virtuel:
.\.venv\Scripts\activate

üìå Tu dois voir :
(.venv) au d√©but de la ligne ‚Üí environnement actif

Ensuite :
‚úîÔ∏è Installer toutes les d√©pendances (requirement.txt)

‚û°Ô∏è dans le dossier tri_state_ai, cr√©e un fichier requirement.txt dans lequel tu colle toutes les informations suivantes: 
'''
# === Core Python Data Science ===
numpy>=1.26
pandas>=2.2
pyarrow>=16.0
fastparquet>=2024.2
openpyxl>=3.1
xlrd>=2.0
scipy>=1.13

# === Visualization ===
matplotlib>=3.9
seaborn>=0.13
plotly>=5.24
missingno>=0.5.2

# === Machine Learning / Fairness / Model Evaluation ===
scikit-learn>=1.5
xgboost>=2.1
statsmodels>=0.14
shap>=0.46
imbalance-learn>=0.12
fairlearn>=0.10.0

# === Geospatial & Mapping (si besoin ACS + carto) ===
geopandas>=1.0
pyproj>=3.6
shapely>=2.0
folium>=0.17

# === Data Engineering / Performance ===
pyarrow>=16.0
numba>=0.60
dask>=2024.8
polars>=1.6.0

# === Download / Files / API ===
requests>=2.32
urllib3>=2.2

# === Notebooks & Documentation ===
jupyter
ipykernel
nbformat>=5.10
nbconvert>=7.16
jupyterlab>=4.2
tqdm>=4.66

# === Logging & Utils ===
python-dotenv>=1.0
loguru>=0.7.2

# === Development & Code Quality ===
black>=24.8
isort>=5.13
flake8>=7.1
mypy>=1.10
pre-commit>=3.8

# === Advanced Option (Text / NLP if futur SMC + sentiment) ===
nltk>=3.9
spacy>=3.7
transformers>=4.43

# === Performance & Compression ===
lz4>=4.3
zstandard>=0.23

'''

‚û°Ô∏è 
pip install -r requirements.txt
Puis :
pip install --upgrade pip

pour encore se rassurer:
pip install --upgrade pip setuptools wheel


################################################

üöÄ √âtape 4 ‚Äî Lancer Jupyter dans l‚Äôenvironnement virtuel

Toujours avec le venv actif :

python -m notebook


‚û°Ô∏è Le navigateur s‚Äôouvre avec Jupyter

A partir de github, on peut uploader le fichier 00_project_setup.ipynb 
puis ex√©cuter les cellule de code avec les adaptations du path du projet
ou alors : 


Cr√©er un notebook dans :
tri_state_ai/notebooks/00_project_setup.ipynb

üß± Premi√®res cellules dans le notebook

Ces blocs configurent automatiquement toute la structure du projet

üìå Cellule 1 ‚Äî V√©rification + chemins de base
import os
import pathlib
import platform

# üß© 1. V√©rification de la configuration syst√®me
print("Syst√®me :", platform.system(), platform.release())
print("Utilisateur :", pathlib.Path.home())

# üóÇÔ∏è 2. Chemins du projet
BASE_DIR  = r"C:\Users\33669\OneDrive\–î–æ–∫—É–º–µ–Ω—Ç—ã\data_credit_scoring"
PROJECT    = os.path.join(BASE_DIR, "tri_state_ai")

# Sous-dossiers du projet
DATA_RAW  = os.path.join(PROJECT, "data_raw", "hmda")
DATA_WORK = os.path.join(PROJECT, "data_work")
NOTEBOOKS = os.path.join(PROJECT, "notebooks")

# Cr√©ation s‚Äôils n‚Äôexistent pas
for path in [DATA_RAW, DATA_WORK, NOTEBOOKS]:
    os.makedirs(path, exist_ok=True)

print("\n‚úÖ Dossiers configur√©s avec succ√®s :")
print(" - Donn√©es brutes :", DATA_RAW)
print(" - Donn√©es trait√©es :", DATA_WORK)
print(" - Notebooks :", NOTEBOOKS)


‚úîÔ∏è R√©sultat attendu :
3 chemins affich√©s + message de succ√®s

üìå Cellule 2 ‚Äî Sp√©cifique Tri-State & confirmation
import os

BASE_DIR   = r"C:\Users\33669\OneDrive\–î–æ–∫—É–º–µ–Ω—Ç—ã\data_credit_scoring"
PROJECT    = os.path.join(BASE_DIR, "tri_state_ai")
DATA_RAW   = os.path.join(PROJECT, "data_raw", "hmda")
DATA_WORK  = os.path.join(PROJECT, "data_work")
NOTEBOOKS  = os.path.join(PROJECT, "notebooks")

# Codes FIPS des √âtats Tri-State : NY=36, NJ=34, CT=09
TRI_STATES = {"36", "34", "09"}

print("‚úÖ Dossiers OK")
print("DATA_RAW :", DATA_RAW)
print("DATA_WORK:", DATA_WORK)
print("TRI-STATES :", TRI_STATES)


‚úîÔ∏è R√©sultat attendu :
Dossiers & codes FIPS confirm√©s

###########################################

Teste l‚Äôimport rapide pour valider en ex√©cutant dans une cellule du notebook :

import numpy as np
import pandas as pd
import seaborn as sns
import sklearn
import xgboost
import statsmodels
import shap
import geopandas as gpd


Si aucune erreur ‚Üí setup 100% valid√©

###########################################

On peut enfin aller sur github pour avoir acc√®s √† tout les code du chap4 et article qu'on ex√©cutera.