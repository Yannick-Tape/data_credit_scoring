# Exécute dans Jupyter si tu n'as pas fait l'installation dans le terminal
%pip install -q pandas pyarrow requests tqdm


######################

import os
os.makedirs("../data_work", exist_ok=True)
os.makedirs("../data_raw/hmda", exist_ok=True)


# TEST: Simple Requête sur API (Etat de NY sur l'année 2021)

import requests, pandas as pd
import pyarrow as pa, pyarrow.parquet as pq

API = "https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations"

params = {
    "years": "2021",
    "states": "NY",
    "actions_taken": "1,2,3,6,7",  # 1=Approved; 2/3/6/7 autres statuts
    # Tu peux ajouter d'autres filtres docs: loan_types, loan_purposes, races, ethnicities, counties, msamds...
}

resp = requests.get(API, params=params, timeout=120)
resp.raise_for_status()  # stoppe si HTTP ≠ 200

js = resp.json()
df = pd.DataFrame(js.get("aggregations", []))
print("Colonnes:", df.columns.tolist())
display(df.head())

pq.write_table(pa.Table.from_pandas(df), "../data_work/hmda_api_sample.parquet")
print("Écrit ../data_work/hmda_api_sample.parquet | lignes:", len(df))


#########################

Super M. NAPO; 

sanity check API fonctionne bien.
on a obtenu un petit tableau d’agrégats pour NY en 2021 écrit en Parquet (c'est adapté pour le big data donc c'est une version csv pour données volumineuse). En nous reférant à la documentation de l'API, voyons ce que ça veut dire, puis on enchaîne proprement vers le row-level (micro-données), indispensable pour la suite (nos modèles ne sont applicables qu'au micro-données).

actions_taken : code HMDA de l’issue de la demande :
    1 = originated / approved (prêt effectivement octroyé)
    2 = approved mais non accepté par l’emprunteur
    3 = refusé
    6 = purchased loan (prêt racheté sur le marché secondaire)
    7 = preapproval denied (pré-approbation refusée)
    count : nombre d’enregistrements correspondant à ce code d’issue, dans le périmètre filtré (NY, 2021 ici).
    sum : somme agrégée d’une métrique que l’API calcule par défaut (typiquement la somme des montants de prêts quand dispo). C’est utile pour une
            sanity check, mais pas exploitable pour nos modèles.

----->  Ces agrégats prouvent que l’API répond, que nos filtres sont valides, et que l’écriture Parquet marche.
----->  Pour entraîner des modèles, mesurer la fairness (et si possible joindre ACS plus tard), il nous faut les micro-données (row-level).


CEPENDANT, NOUS AVONS RENCONTRER UN PROBLEME ET AVONS RESOLUE COMME SUIT :

L’API HMDA Data Browser n’accepte pas toutes les années. D’après la doc officielle, le filtre years est limité aux millésimes récents (2018, 2019, 2020, 2021, 2022) pour l’API Data Browser (les années plus anciennes sont à récupérer via les static datasets et non via cet endpoint). Donc appeler l’API avec years=2004 (par exemple) déclenche une Error 400.

Pour les années de 2004 à 2017 ont utilisera les fichiers historiques (bulk/static datasets) au lieu de l’API Data Browser. Pour 2018 jusqu'à 2022 normalement (si possible, étendre à avec 2023 et 2024), l’API marche très bien pour les agrégats rapides et des extractions ciblées.



#################

# Commençons par restreindre la boucle aux années supportées par l’API

import time, requests, pandas as pd
from tqdm import tqdm

API = "https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations"

# Années acceptées par l'API Data Browser d'après la doc. On a essayer d'étendre à avec 2023 et 2024 
# avec succès donc l'API les a ajoutées

API_YEARS = [2018, 2019, 2020, 2021, 2022, 2023, 2024]  # essaie ensuite d'ajouter 2023, 2024 si nécessaire
STATES = ["NY","NJ","CT"]

rows = []
for year in API_YEARS:
    for st in STATES:
        params = {
            "years": str(year),
            "states": st,
            "actions_taken": "1,2,3,6,7",
        }
        try:
            r = requests.get(API, params=params, timeout=120)
            if r.status_code == 429:
                # Trop de requêtes -> on attend un peu et on retente une fois
                time.sleep(2)
                r = requests.get(API, params=params, timeout=120)
            r.raise_for_status()
            data = r.json().get("aggregations", [])
            for rec in data:
                rec["year"] = year
                rec["state"] = st
                rows.append(rec)
        except requests.HTTPError as e:
            print(f"Skip {year} {st}: {e}")
            continue

agg_tri = pd.DataFrame(rows)
agg_tri.to_parquet("../data_work/hmda_api_aggregates_tristate_2018_2022.parquet", index=False)
print("écrit agrégats:", agg_tri.shape)
agg_tri.head()


#############


# Commençons par restreindre la boucle aux années supportées par l’API

import time, requests, pandas as pd
from tqdm import tqdm

API = "https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations"

# Années acceptées par l'API Data Browser d'après la doc. On a essayer d'étendre à avec 2023 et 2024 
# avec succès donc l'API les a ajoutées

API_YEARS = [2018, 2019, 2020, 2021, 2022, 2023, 2024]  # essaie ensuite d'ajouter 2023, 2024 si nécessaire
STATES = ["NY","NJ","CT"]

rows = []
for year in API_YEARS:
    for st in STATES:
        params = {
            "years": str(year),
            "states": st,
            "actions_taken": "1,2,3,6,7",
        }
        try:
            r = requests.get(API, params=params, timeout=120)
            if r.status_code == 429:
                # Trop de requêtes -> on attend un peu et on retente une fois
                time.sleep(2)
                r = requests.get(API, params=params, timeout=120)
            r.raise_for_status()
            data = r.json().get("aggregations", [])
            for rec in data:
                rec["year"] = year
                rec["state"] = st
                rows.append(rec)
        except requests.HTTPError as e:
            print(f"Skip {year} {st}: {e}")
            continue

agg_tri = pd.DataFrame(rows)
agg_tri.to_parquet("../data_work/hmda_api_aggregates_tristate_2018_2024.parquet", index=False)
print("écrit agrégats:", agg_tri.shape)
agg_tri.head()


#################


# contrôles indispensables (qualité de base)

# 1) années et états réellement présents
print("Années:", sorted(agg_tri['year'].unique()))
print("États :", sorted(agg_tri['state'].unique()))

# 2) on s’attend à 5 lignes par (année, État) = 5 codes actions_taken (1,2,3,6,7)
chk = agg_tri.groupby(['year','state'])['actions_taken'].nunique().reset_index(name='n_codes')
display(chk.sort_values(['year','state']))  # doit être = 5 partout

# 3) check de valeurs manquantes
display(agg_tri.isna().mean().to_frame('missing_ratio'))


################


Ces agrégats nous ont énormement renseigné sur l'API puisque nous savons ce qu'il est possible d'en extraire. Nous venons de vérifier l’API et faire un « coup d’œil » macro. Pour la suite (modèles, fairness, fusion ACS), on a besoin des micro-données (row-level).

Nous récupérons plus tard les historiques “bulk/static” 2004–2017 

Nous allons utiliser le HMDA Data Science Kit du CFPB, qui fournit des scripts de téléchargement automatisés pour les fichiers LAR (Loan Application Register), Panel et Transmittal Sheet de 2004 à 2022, avec des scripts de décompression et des exemples d’usage prêts à l’emploi.
 
 Le README précise qu’il faut télécharger manuellement 2015–2016 (site FFIEC non “scrapable”) : les scripts skip ces années si les fichiers ne sont pas déjà présents.


