# Ex√©cute dans Jupyter si tu n'as pas fait l'installation dans le terminal
%pip install -q pandas pyarrow requests tqdm


######################

import os
os.makedirs("../data_work", exist_ok=True)
os.makedirs("../data_raw/hmda", exist_ok=True)


# TEST: Simple Requ√™te sur API (Etat de NY sur l'ann√©e 2021)

import requests, pandas as pd
import pyarrow as pa, pyarrow.parquet as pq

API = "https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations"

params = {
    "years": "2021",
    "states": "NY",
    "actions_taken": "1,2,3,6,7",  # 1=Approved; 2/3/6/7 autres statuts
    # Tu peux ajouter d'autres filtres docs: loan_types, loan_purposes, races, ethnicities, counties, msamds...
}

resp = requests.get(API, params=params, timeout=120)
resp.raise_for_status()  # stoppe si HTTP ‚â† 200

js = resp.json()
df = pd.DataFrame(js.get("aggregations", []))
print("Colonnes:", df.columns.tolist())
display(df.head())

pq.write_table(pa.Table.from_pandas(df), "../data_work/hmda_api_sample.parquet")
print("√âcrit ../data_work/hmda_api_sample.parquet | lignes:", len(df))


#########################

Super M. NAPO; 

sanity check API fonctionne bien.
on a obtenu un petit tableau d‚Äôagr√©gats pour NY en 2021 √©crit en Parquet (c'est adapt√© pour le big data donc c'est une version csv pour donn√©es volumineuse). En nous ref√©rant √† la documentation de l'API, voyons ce que √ßa veut dire, puis on encha√Æne proprement vers le row-level (micro-donn√©es), indispensable pour la suite (nos mod√®les ne sont applicables qu'au micro-donn√©es).

actions_taken : code HMDA de l‚Äôissue de la demande :
    1 = originated / approved (pr√™t effectivement octroy√©)
    2 = approved mais non accept√© par l‚Äôemprunteur
    3 = refus√©
    6 = purchased loan (pr√™t rachet√© sur le march√© secondaire)
    7 = preapproval denied (pr√©-approbation refus√©e)
    count : nombre d‚Äôenregistrements correspondant √† ce code d‚Äôissue, dans le p√©rim√®tre filtr√© (NY, 2021 ici).
    sum : somme agr√©g√©e d‚Äôune m√©trique que l‚ÄôAPI calcule par d√©faut (typiquement la somme des montants de pr√™ts quand dispo). C‚Äôest utile pour une
            sanity check, mais pas exploitable pour nos mod√®les.

----->  Ces agr√©gats prouvent que l‚ÄôAPI r√©pond, que nos filtres sont valides, et que l‚Äô√©criture Parquet marche.
----->  Pour entra√Æner des mod√®les, mesurer la fairness (et si possible joindre ACS plus tard), il nous faut les micro-donn√©es (row-level).


CEPENDANT, NOUS AVONS RENCONTRER UN PROBLEME ET AVONS RESOLUE COMME SUIT :

L‚ÄôAPI HMDA Data Browser n‚Äôaccepte pas toutes les ann√©es. D‚Äôapr√®s la doc officielle, le filtre years est limit√© aux mill√©simes r√©cents (2018, 2019, 2020, 2021, 2022) pour l‚ÄôAPI Data Browser (les ann√©es plus anciennes sont √† r√©cup√©rer via les static datasets et non via cet endpoint). Donc appeler l‚ÄôAPI avec years=2004 (par exemple) d√©clenche une Error 400.

Pour les ann√©es de 2004 √† 2017 ont utilisera les fichiers historiques (bulk/static datasets) au lieu de l‚ÄôAPI Data Browser. Pour 2018 jusqu'√† 2022 normalement (si possible, √©tendre √† avec 2023 et 2024), l‚ÄôAPI marche tr√®s bien pour les agr√©gats rapides et des extractions cibl√©es.



#################

# Commen√ßons par restreindre la boucle aux ann√©es support√©es par l‚ÄôAPI

import time, requests, pandas as pd
from tqdm import tqdm

API = "https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations"

# Ann√©es accept√©es par l'API Data Browser d'apr√®s la doc. On a essayer d'√©tendre √† avec 2023 et 2024 
# avec succ√®s donc l'API les a ajout√©es

API_YEARS = [2018, 2019, 2020, 2021, 2022, 2023, 2024]  # essaie ensuite d'ajouter 2023, 2024 si n√©cessaire
STATES = ["NY","NJ","CT"]

rows = []
for year in API_YEARS:
    for st in STATES:
        params = {
            "years": str(year),
            "states": st,
            "actions_taken": "1,2,3,6,7",
        }
        try:
            r = requests.get(API, params=params, timeout=120)
            if r.status_code == 429:
                # Trop de requ√™tes -> on attend un peu et on retente une fois
                time.sleep(2)
                r = requests.get(API, params=params, timeout=120)
            r.raise_for_status()
            data = r.json().get("aggregations", [])
            for rec in data:
                rec["year"] = year
                rec["state"] = st
                rows.append(rec)
        except requests.HTTPError as e:
            print(f"Skip {year} {st}: {e}")
            continue

agg_tri = pd.DataFrame(rows)
agg_tri.to_parquet("../data_work/hmda_api_aggregates_tristate_2018_2022.parquet", index=False)
print("√©crit agr√©gats:", agg_tri.shape)
agg_tri.head()


#############


# Commen√ßons par restreindre la boucle aux ann√©es support√©es par l‚ÄôAPI

import time, requests, pandas as pd
from tqdm import tqdm

API = "https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations"

# Ann√©es accept√©es par l'API Data Browser d'apr√®s la doc. On a essayer d'√©tendre √† avec 2023 et 2024 
# avec succ√®s donc l'API les a ajout√©es

API_YEARS = [2018, 2019, 2020, 2021, 2022, 2023, 2024]  # essaie ensuite d'ajouter 2023, 2024 si n√©cessaire
STATES = ["NY","NJ","CT"]

rows = []
for year in API_YEARS:
    for st in STATES:
        params = {
            "years": str(year),
            "states": st,
            "actions_taken": "1,2,3,6,7",
        }
        try:
            r = requests.get(API, params=params, timeout=120)
            if r.status_code == 429:
                # Trop de requ√™tes -> on attend un peu et on retente une fois
                time.sleep(2)
                r = requests.get(API, params=params, timeout=120)
            r.raise_for_status()
            data = r.json().get("aggregations", [])
            for rec in data:
                rec["year"] = year
                rec["state"] = st
                rows.append(rec)
        except requests.HTTPError as e:
            print(f"Skip {year} {st}: {e}")
            continue

agg_tri = pd.DataFrame(rows)
agg_tri.to_parquet("../data_work/hmda_api_aggregates_tristate_2018_2024.parquet", index=False)
print("√©crit agr√©gats:", agg_tri.shape)
agg_tri.head()


#################


# contr√¥les indispensables (qualit√© de base)

# 1) ann√©es et √©tats r√©ellement pr√©sents
print("Ann√©es:", sorted(agg_tri['year'].unique()))
print("√âtats :", sorted(agg_tri['state'].unique()))

# 2) on s‚Äôattend √† 5 lignes par (ann√©e, √âtat) = 5 codes actions_taken (1,2,3,6,7)
chk = agg_tri.groupby(['year','state'])['actions_taken'].nunique().reset_index(name='n_codes')
display(chk.sort_values(['year','state']))  # doit √™tre = 5 partout

# 3) check de valeurs manquantes
display(agg_tri.isna().mean().to_frame('missing_ratio'))


################


Ces agr√©gats nous ont √©normement renseign√© sur l'API puisque nous savons ce qu'il est possible d'en extraire. Nous venons de v√©rifier l‚ÄôAPI et faire un ¬´ coup d‚Äô≈ìil ¬ª macro. Pour la suite (mod√®les, fairness, fusion ACS), on a besoin des micro-donn√©es (row-level).

R√©cup√©rons les historiques ‚Äúbulk/static‚Äù 2004‚Äì2017 grace aux ligne aux lignes de code ci-dessous

Nous allons utiliser le HMDA Data Science Kit du CFPB, qui fournit des scripts de t√©l√©chargement automatis√©s pour les fichiers LAR (Loan Application Register), Panel et Transmittal Sheet de 2004 √† 2022, avec des scripts de d√©compression et des exemples d‚Äôusage pr√™ts √† l‚Äôemploi.
 
 Le README pr√©cise qu‚Äôil faut t√©l√©charger manuellement 2015‚Äì2016 (site FFIEC non ‚Äúscrapable‚Äù) : les scripts skip ces ann√©es si les fichiers ne sont pas d√©j√† pr√©sents.


##################

import os, pathlib, glob, textwrap, json, subprocess

HOME = str(pathlib.Path("~").expanduser())
proj = os.path.join(HOME, "tri_state_ai")
kit_dir = os.path.join(proj, "HMDA_Data_Science_Kit")

print("KIT_DIR:", kit_dir)
print("\nContenu racine (30 premiers √©l√©ments):")
for p in sorted(os.listdir(kit_dir))[:30]:
    print(" -", p)

# Chercher un README* n'importe o√π (md, rst, txt‚Ä¶)
readmes = []
for pat in ("README*", "readme*", "*.rst", "*.md", "*.txt"):
    readmes.extend(glob.glob(os.path.join(kit_dir, "**", pat), recursive=True))
readmes = [p for p in readmes if os.path.isfile(p) and "LICENSE" not in os.path.basename(p)]
print("\nREADMEs possibles trouv√©s:", len(readmes))
for p in sorted(readmes)[:10]:
    print(" -", p)

# Chercher les scripts utiles
script_candidates = []
for pat in ("download*.sh", "unzip*.sh", "*.py"):
    script_candidates.extend(glob.glob(os.path.join(kit_dir, "**", pat), recursive=True))

# Filtrer sur les noms probables
dl_scripts = [p for p in script_candidates if os.path.basename(p) in (
    "download_hmda.sh", "unzip_all.sh", "unzip_and_rename_lar.sh"
)]
print("\nScripts de t√©l√©chargement/d√©compression rep√©r√©s:", len(dl_scripts))
for p in sorted(dl_scripts):
    print(" -", p)


#######################

# On se place dans le dossier des scripts
%cd "/home/ubuntu/tri_state_ai/HMDA_Data_Science_Kit/download_scripts"

# (le script r√©cup√®re tout ce qui manque ; s'il ne peut pas prendre certains mill√©simes, il les "skippe" et l'indique)
!bash download_hmda.sh -l


####################

R√©cup√©rons l‚ÄôURL par ann√©e (depuis le d√©p√¥t du kit)

Le d√©p√¥t que qu'on a clon√© contient hmda_data_links.md avec les liens officiels. On va extraire automatiquement l‚ÄôURL LAR par ann√©e, puis traiter ann√©e par ann√©e en √©conomisant le disque.

# 1) Afficher les 200 premi√®res lignes de hmda_data_links.md
links_file = "/home/ubuntu/tri_state_ai/HMDA_Data_Science_Kit/hmda_data_links.md"
with open(links_file, "r", encoding="utf-8", errors="ignore") as f:
    for i, line in enumerate(f):
        if i > 200: break
        print(f"{i+1:03}: {line.rstrip()}")

# 2) Extraire toutes les URLs visibles (peu importe l'extension) pour voir le pattern r√©el
import re
with open(links_file, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

all_urls = re.findall(r"https?://[^\s\)\]]+", text)
print("Nb d'URL trouv√©es:", len(all_urls))
for u in all_urls[:20]:
    print(" -", u)

# 3) Chercher dans TOUT le d√©p√¥t des patterns utiles (.z/.zip + 'lar' + ann√©e)
import glob, os
repo = "/home/ubuntu/tri_state_ai/HMDA_Data_Science_Kit"
cands = glob.glob(os.path.join(repo, "**", "*.*"), recursive=True)
hits = []
pat = re.compile(r"https?://[^\s\)\]]+(lar|LAR)[^\s\)\]]+(19|20)\d{2}\.(z|Z|zip|ZIP)")
for p in cands:
    # lire de petits fichiers texte seulement
    try:
        if os.path.getsize(p) > 2_000_000:
            continue
        with open(p, "r", encoding="utf-8", errors="ignore") as f:
            txt = f.read()
        for m in pat.findall(txt):
            hits.append((p, m))
    except:
        pass

print("Hits potentiels (fichiers contenant des URLs lar_YYYY.zip/.z):", len(hits))
for h in hits[:10]:
    print(" -", h[0], "‚Üí", h[1])


####################


import os, glob, pathlib

HOME = str(pathlib.Path("~").expanduser())
kit_dir = os.path.join(HOME, "tri_state_ai", "HMDA_Data_Science_Kit")
z_dir   = os.path.join(kit_dir, "download_scripts", "data", "lar")  # o√π le script met les .z

z_files = sorted(glob.glob(os.path.join(z_dir, "lar_*.z")))
print("Fichiers .z d√©tect√©s:", len(z_files))
for p in z_files[:10]:
    sz = os.path.getsize(p)/1024/1024
    print(f" - {os.path.basename(p)}  ~{sz:.1f} MB")

print("\nAstuce: on traitera 1 fichier ‚Üí on √©crit ‚Üí on supprime ‚Üí suivant (√©conomie d‚Äôespace).")


####################


import requests, os, pathlib

HOME = str(pathlib.Path("~").expanduser())
dest = os.path.join(HOME, "tri_state_ai", "data_raw", "hmda")
os.makedirs(dest, exist_ok=True)

# Exemple pour 2004 :
url = "https://catalog.archives.gov/download/5716418"  # (Final LAR 2004)
out = os.path.join(dest, "lar_2004.zip")

print("T√©l√©chargement :", url)
r = requests.get(url, stream=True, timeout=600)
r.raise_for_status()
with open(out, "wb") as f:
    for chunk in r.iter_content(chunk_size=1024*1024):
        if chunk:
            f.write(chunk)
print("Enregistr√© :", out, f"({os.path.getsize(out)/1024/1024:.1f} MB)")

####################

on va √©crire une cellule compl√®te Python pr√™te √† ex√©cuter dans ton notebook, qui t√©l√©charge automatiquement toutes les ann√©es 2004‚Äì2017 depuis les archives nationales am√©ricaines (NARA), selon les identifiants d√©j√† pr√©sents dans notre fichier hmda_data_links.md.



# === HMDA 2017 ‚Äî ALL RECORDS (CFPB) ===
# Objectifs :
#  1) T√©l√©charger l‚Äôarchive "hmda_2017_nationwide_all-records_labels.zip" (All records) depuis CFPB
#  2) Calculer une empreinte SHA-256 pour la tra√ßabilit√© (optionnel mais pro)
#  3) D√©compresser dans ~/tri_state_ai/data_raw/hmda/
#  4) Lister les fichiers extraits (CSV/TXT)

import os, pathlib, requests, hashlib, zipfile

HOME = str(pathlib.Path("~").expanduser())
dest_dir = os.path.join(HOME, "tri_state_ai", "data_raw", "hmda")
os.makedirs(dest_dir, exist_ok=True)

# URL officielle "All records" 2017 (h√©berg√©e par CFPB)
# R√©f. nom de fichier : "hmda_2017_nationwide_all-records_labels.zip"
# Voir pages "Download HMDA data (2007‚Äì2017)" et mirroirs (DataLumos/OpenICPSR).
url = "https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_nationwide_all-records_labels.zip"

out_zip = os.path.join(dest_dir, "hmda_2017_nationwide_all-records_labels.zip")

# 1) T√©l√©chargement (avec reprise si d√©j√† pr√©sent)
if not os.path.exists(out_zip):
    print("‚¨áÔ∏è  T√©l√©chargement :", url)
    with requests.get(url, stream=True, timeout=600) as r:
        r.raise_for_status()
        with open(out_zip, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024*1024):
                if chunk:
                    f.write(chunk)
else:
    print("‚è© Fichier d√©j√† pr√©sent :", out_zip)

# Taille de l‚Äôarchive
size_mb = os.path.getsize(out_zip)/1024/1024
print(f"‚úÖ Archive : {out_zip}  ({size_mb:.1f} MB)")

# 2) Empreinte SHA-256 (pour journaliser la source exacte utilis√©e)
h = hashlib.sha256()
with open(out_zip, "rb") as f:
    for chunk in iter(lambda: f.read(1024*1024), b""):
        h.update(chunk)
sha256 = h.hexdigest()
print("SHA256 :", sha256)

# 3) D√©compression
print("üì¶ D√©compression‚Ä¶")
with zipfile.ZipFile(out_zip, "r") as z:
    z.extractall(dest_dir)
print("‚úÖ D√©compress√© dans :", dest_dir)

# 4) Lister les fichiers HMDA 2017 extraits
print("\nFichiers 2017 extraits :")
for fn in sorted(os.listdir(dest_dir)):
    if fn.lower().startswith("hmda_2017"):
        print(" -", fn)

#################### On viens d'obtenir un bug ci dessus √† cause de notre capacit√© de stockage limit√© de notre VM


import zipfile, io, pandas as pd, pyarrow as pa, pyarrow.parquet as pq, os, pathlib

HOME = str(pathlib.Path("~").expanduser())
zip_path = os.path.join(HOME, "tri_state_ai", "data_raw", "hmda", "hmda_2017_nationwide_all-records_labels.zip")
out_parquet = os.path.join(HOME, "tri_state_ai", "data_work", "hmda_2017_tristate.parquet")
tri_states = {"36","34","09"}

with zipfile.ZipFile(zip_path) as z:
    csv_name = [n for n in z.namelist() if n.lower().endswith((".csv",".txt"))][0]
    print("Lecture stream :", csv_name)
    with z.open(csv_name) as f:
        reader = pd.read_csv(f, chunksize=200_000, low_memory=False)
        total = 0
        for chunk in reader:
            cols = [c.strip().lower() for c in chunk.columns]
            chunk.columns = cols
            # trouver colonne d'√©tat
            state_col = None
            for c in ["state_code","state","st","statefips"]:
                if c in chunk.columns:
                    state_col = c; break
            if state_col:
                sub = chunk[chunk[state_col].astype(str).str.replace(r"\D","",regex=True).isin(tri_states)]
            else:
                sub = chunk
            if len(sub):
                sub["year"] = 2017
                table = pa.Table.from_pandas(sub, preserve_index=False)
                if not os.path.exists(out_parquet):
                    pq.write_table(table, out_parquet)
                else:
                    with pq.ParquetWriter(out_parquet, table.schema, use_dictionary=True) as writer:
                        writer.write_table(table)
                total += len(sub)
        print(f"‚úÖ {total:,} lignes Tri-State sauvegard√©es dans {out_parquet}")