{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db2fd43-7990-44a8-8e4e-11121373e4cc",
   "metadata": {},
   "source": [
    "**But de ce notebook (Étape 1.1 · HMDA) :**<br>  *1. vérifier l’API (agrégats, 2018+)*<br>\n",
    "    *2. télécharger les datasets historiques (bulk/static) 2004–2017 et ceux de l'API (2018 et +)*<br>\n",
    "    *3. tous les ramener en tri-states (NY, NJ, Connecticut)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d956c-8fe9-4301-99ef-c0e2393a5604",
   "metadata": {},
   "source": [
    "**1 Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77136240-e170-4bdc-b952-d00e1cb49b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas requests pyarrow --quiet   # à exécuter si pandas et requests ne sont pas déjà installés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37294186-2257-4295-ba2c-9b1e68ab2a7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dossiers OK\n",
      "DATA_RAW : C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\n",
      "DATA_WORK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR   = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\"\n",
    "PROJECT    = os.path.join(BASE_DIR, \"tri_state_ai\")\n",
    "DATA_RAW   = os.path.join(PROJECT, \"data_raw\", \"hmda\")   # là où on stocke les ZIP/CSV bruts\n",
    "DATA_WORK  = os.path.join(PROJECT, \"data_work\")          # sorties Parquet\n",
    "NOTEBOOKS  = os.path.join(PROJECT, \"notebooks\")\n",
    "\n",
    "for p in [DATA_RAW, DATA_WORK, NOTEBOOKS]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# Codes FIPS des États Tri-State : NY=36, NJ=34, CT=09\n",
    "TRI_STATES = {\"36\", \"34\", \"09\"}\n",
    "\n",
    "print(\"✅ Dossiers OK\")\n",
    "print(\"DATA_RAW :\", DATA_RAW)\n",
    "print(\"DATA_WORK:\", DATA_WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2020640-ed78-40b5-bd87-dd6cea2aa6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ca796-2295-4811-9b66-dc48f747c68a",
   "metadata": {},
   "source": [
    "**2 Sanity check API (agrégats, 2018+ seulement)**<br>\n",
    "*But: tester que l’API répond et que le schéma attendu est correct (GET + params).*<br>\n",
    "⚠️ *L’API “data-browser” ne renvoie que des agrégats, pas les lignes brutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b593cfef-c943-47e0-9bd8-d5b2c014c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas requests --quiet  # à exécuter si pandas et requests ne sont pas déjà installés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4693c0f-8aae-44c8-93f4-f18349d13897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th>actions_taken</th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13830</td>\n",
       "      <td>4.910830e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110166</td>\n",
       "      <td>2.614173e+10</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>1.582500e+08</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60253</td>\n",
       "      <td>1.664848e+10</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>283988</td>\n",
       "      <td>1.123445e+11</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count           sum actions_taken  year state\n",
       "0   13830  4.910830e+09             2  2018    NY\n",
       "1  110166  2.614173e+10             3  2018    NY\n",
       "2     288  1.582500e+08             7  2018    NY\n",
       "3   60253  1.664848e+10             6  2018    NY\n",
       "4  283988  1.123445e+11             1  2018    NY"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, pandas as pd\n",
    "\n",
    "API = \"https://ffiec.cfpb.gov/v2/data-browser-api/view/aggregations\"\n",
    "years  = [2018, 2019, 2020, 2021, 2022]  # l’API supporte 2018+ (on reste léger ici)\n",
    "states = [\"NY\",\"NJ\",\"CT\"]\n",
    "\n",
    "rows = []\n",
    "for y in years:\n",
    "    for st in states:\n",
    "        params = {\n",
    "            \"years\": str(y),\n",
    "            \"states\": st,\n",
    "            \"actions_taken\": \"1,2,3,6,7\",  # 1=Approved + autres statuts utiles\n",
    "        }\n",
    "        r = requests.get(API, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        for rec in r.json().get(\"aggregations\", []):\n",
    "            rec[\"year\"] = y\n",
    "            rec[\"state\"] = st\n",
    "            rows.append(rec)\n",
    "\n",
    "df_api = pd.DataFrame(rows)\n",
    "print(df_api.shape)\n",
    "df_api.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb422f-dcc1-4d2e-b83e-b19863fc4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606a628-f7c9-4c3e-8879-ef85c7c9fc66",
   "metadata": {},
   "source": [
    "**3 Téléchargement des historiques 2004–2017 (bulk/static)**<br>\n",
    "But: ici ont construit une base locale fiable et traçable de toutes les archives HMDA 2004–2017, prêtes pour le traitement streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edea5ee-7e9e-4aca-acaa-103dc93e6c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  2017 (All records)…\n",
      "✅ téléchargé : C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_2017_nationwide_all-records_labels.zip\n",
      "SHA256: f43dbc4f1a674d8ae21a3a4f0a07f0a1366063f695724a564c867498046f2984\n"
     ]
    }
   ],
   "source": [
    "# 2017 — “All records” (CFPB/FFIEC)\n",
    "\n",
    "import os, hashlib, requests\n",
    "\n",
    "url_2017_all = \"https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_nationwide_all-records_labels.zip\"\n",
    "out_2017_zip = os.path.join(DATA_RAW, \"hmda_2017_nationwide_all-records_labels.zip\")\n",
    "\n",
    "if not os.path.exists(out_2017_zip):\n",
    "    print(\"⬇️  2017 (All records)…\")\n",
    "    with requests.get(url_2017_all, stream=True, timeout=600) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(out_2017_zip, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                if chunk: f.write(chunk)\n",
    "    print(\"✅ téléchargé :\", out_2017_zip)\n",
    "else:\n",
    "    print(\"⏭️ déjà présent :\", out_2017_zip)\n",
    "\n",
    "# Empreinte (traçabilité)\n",
    "h = hashlib.sha256()\n",
    "with open(out_2017_zip, \"rb\") as f:\n",
    "    for ch in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "        h.update(ch)\n",
    "print(\"SHA256:\", h.hexdigest())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95dd0021-9fe2-4c62-b6b4-c51afab9c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  2004 → https://catalog.archives.gov/download/5716418\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2004.zip\n",
      "⬇️  2005 → https://catalog.archives.gov/download/6850582\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2005.zip\n",
      "⬇️  2006 → https://catalog.archives.gov/download/6850584\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2006.zip\n",
      "⬇️  2007 → https://catalog.archives.gov/download/6852883\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2007.zip\n",
      "⬇️  2008 → https://catalog.archives.gov/download/6852884\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2008.zip\n",
      "⬇️  2009 → https://catalog.archives.gov/download/6872010\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2009.zip\n",
      "⬇️  2010 → https://catalog.archives.gov/download/12008309\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2010.zip\n",
      "⬇️  2011 → https://catalog.archives.gov/download/12008312\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2011.zip\n",
      "⬇️  2012 → https://catalog.archives.gov/download/18491490\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2012.zip\n",
      "⬇️  2013 → https://catalog.archives.gov/download/34618164\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2013.zip\n",
      "⬇️  2014 → https://catalog.archives.gov/download/100378087\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2014.zip\n",
      "⬇️  2015 → https://catalog.archives.gov/download/5752989\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2015.zip\n",
      "⬇️  2016 → https://catalog.archives.gov/download/5752994\n",
      "  ✅ OK: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_lar_2016.zip\n"
     ]
    }
   ],
   "source": [
    "# 2004–2016 — LAR “Final” via NARA (National Archives):\n",
    "  # On utilise les ID NARA officiels pour construire les liens https://catalog.archives.gov/download/<ID>\n",
    "\n",
    "import time\n",
    "\n",
    "nara_ids = {\n",
    "    2004: 5716418,  2005: 6850582,  2006: 6850584,  2007: 6852883,\n",
    "    2008: 6852884,  2009: 6872010,  2010: 12008309, 2011: 12008312,\n",
    "    2012: 18491490, 2013: 34618164, 2014: 100378087, 2015: 5752989,\n",
    "    2016: 5752994,\n",
    "}\n",
    "\n",
    "for year, nid in nara_ids.items():\n",
    "    url = f\"https://catalog.archives.gov/download/{nid}\"\n",
    "    out = os.path.join(DATA_RAW, f\"hmda_lar_{year}.zip\")\n",
    "    if os.path.exists(out):\n",
    "        print(f\"⏭️ {year} déjà présent\")\n",
    "        continue\n",
    "    try:\n",
    "        print(f\"⬇️  {year} → {url}\")\n",
    "        with requests.get(url, stream=True, timeout=600) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(out, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                    if chunk: f.write(chunk)\n",
    "        print(\"  ✅ OK:\", out)\n",
    "        time.sleep(1.5)\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ {year} erreur:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569146e-fd26-4ee7-8c6d-98cae8d7c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c8d0b-6afd-4b07-839c-13db99395f87",
   "metadata": {},
   "source": [
    "**C'est dommage de constaté que tous ces dossiers contenu dans le web et téléchargés avec succès sont corompus.<br> En fait il ne sont pas réelement des .zip et finalement ils sont inexploitables.<br> Nous avous recuperer manuellement les dossiers .zip de bonne source (malgré le fait que leur contenu .excel sont bizarement très lourds en terme de Giga) puis faire une extraction et enfin les stocker en format .parquet pret pour analyses ultérieures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef197118-c634-4e07-9905-6824a3636681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hmda_2007_nationwide_all-records_labels.zip',\n",
       " 'hmda_2008_nationwide_all-records_labels.zip',\n",
       " 'hmda_2009_nationwide_all-records_labels.zip',\n",
       " 'hmda_2010_nationwide_all-records_labels.zip',\n",
       " 'hmda_2011_nationwide_all-records_labels.zip',\n",
       " 'hmda_2012_nationwide_all-records_labels.zip',\n",
       " 'hmda_2013_nationwide_all-records_labels.zip',\n",
       " 'hmda_2014_nationwide_all-records_labels.zip',\n",
       " 'hmda_2015_nationwide_all-records_labels.zip',\n",
       " 'hmda_2016_nationwide_all-records_labels.zip',\n",
       " 'hmda_2017_nationwide_all-records_labels.zip']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nous pouvons voir la liste des dossiers que nous avons télécharges en local\n",
    "DATA_RAW = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\"\n",
    "[z for z in os.listdir(DATA_RAW) if z.endswith(\".zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cfdfef4-b2c8-461e-973d-3ea0aa1ac351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2007 | Source: hmda_2007_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2007\\hmda_2007_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2007: 26,605,695 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2007.parquet\n",
      "\n",
      "=== 2008 | Source: hmda_2008_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2008\\hmda_2008_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2008: 17,391,570 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2008.parquet\n",
      "\n",
      "=== 2009 | Source: hmda_2009_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2009\\hmda_2009_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2009: 19,493,491 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2009.parquet\n",
      "\n",
      "=== 2010 | Source: hmda_2010_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2010\\hmda_2010_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "❌ 2010: Table schema does not match schema used to create file: \n",
      "table:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: int64\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: string\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10596 vs. \n",
      "file:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: string\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10600\n",
      "\n",
      "=== 2011 | Source: hmda_2011_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2011\\hmda_2011_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2011: 14,873,415 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2011.parquet\n",
      "\n",
      "=== 2012 | Source: hmda_2012_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2012\\hmda_2012_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2012: 18,691,551 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2012.parquet\n",
      "\n",
      "=== 2013 | Source: hmda_2013_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2013\\hmda_2013_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "❌ 2013: Table schema does not match schema used to create file: \n",
      "table:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: double\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: double\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: double\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10603 vs. \n",
      "file:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: string\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10600\n",
      "\n",
      "=== 2014 | Source: hmda_2014_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2014\\hmda_2014_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "❌ 2014: Table schema does not match schema used to create file: \n",
      "table:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: double\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10601 vs. \n",
      "file:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: string\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10600\n",
      "\n",
      "=== 2015 | Source: hmda_2015_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2015\\hmda_2015_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "❌ 2015: Table schema does not match schema used to create file: \n",
      "table:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: double\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10601 vs. \n",
      "file:\n",
      "as_of_year: int64\n",
      "respondent_id: string\n",
      "agency_name: string\n",
      "agency_abbr: string\n",
      "agency_code: int64\n",
      "loan_type_name: string\n",
      "loan_type: int64\n",
      "property_type_name: string\n",
      "property_type: int64\n",
      "loan_purpose_name: string\n",
      "loan_purpose: int64\n",
      "owner_occupancy_name: string\n",
      "owner_occupancy: int64\n",
      "loan_amount_000s: int64\n",
      "preapproval_name: string\n",
      "preapproval: int64\n",
      "action_taken_name: string\n",
      "action_taken: int64\n",
      "msamd_name: string\n",
      "msamd: double\n",
      "state_name: string\n",
      "state_abbr: string\n",
      "state_code: double\n",
      "county_name: string\n",
      "county_code: double\n",
      "census_tract_number: double\n",
      "applicant_ethnicity_name: string\n",
      "applicant_ethnicity: int64\n",
      "co_applicant_ethnicity_name: string\n",
      "co_applicant_ethnicity: int64\n",
      "applicant_race_name_1: string\n",
      "applicant_race_1: int64\n",
      "applicant_race_name_2: string\n",
      "applicant_race_2: double\n",
      "applicant_race_name_3: string\n",
      "applicant_race_3: double\n",
      "applicant_race_name_4: string\n",
      "applicant_race_4: double\n",
      "applicant_race_name_5: string\n",
      "applicant_race_5: double\n",
      "co_applicant_race_name_1: string\n",
      "co_applicant_race_1: int64\n",
      "co_applicant_race_name_2: string\n",
      "co_applicant_race_2: double\n",
      "co_applicant_race_name_3: string\n",
      "co_applicant_race_3: double\n",
      "co_applicant_race_name_4: string\n",
      "co_applicant_race_4: double\n",
      "co_applicant_race_name_5: string\n",
      "co_applicant_race_5: double\n",
      "applicant_sex_name: string\n",
      "applicant_sex: int64\n",
      "co_applicant_sex_name: string\n",
      "co_applicant_sex: int64\n",
      "applicant_income_000s: double\n",
      "purchaser_type_name: string\n",
      "purchaser_type: int64\n",
      "denial_reason_name_1: string\n",
      "denial_reason_1: double\n",
      "denial_reason_name_2: string\n",
      "denial_reason_2: double\n",
      "denial_reason_name_3: string\n",
      "denial_reason_3: double\n",
      "rate_spread: double\n",
      "hoepa_status_name: string\n",
      "hoepa_status: int64\n",
      "lien_status_name: string\n",
      "lien_status: int64\n",
      "edit_status_name: string\n",
      "edit_status: double\n",
      "sequence_number: int64\n",
      "population: double\n",
      "minority_population: double\n",
      "hud_median_family_income: double\n",
      "tract_to_msamd_income: double\n",
      "number_of_owner_occupied_units: double\n",
      "number_of_1_to_4_family_units: double\n",
      "application_date_indicator: int64\n",
      "year: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 10600\n",
      "\n",
      "=== 2016 | Source: hmda_2016_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2016\\hmda_2016_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2016: 16,332,987 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2016.parquet\n",
      "\n",
      "=== 2017 | Source: hmda_2017_nationwide_all-records_labels.zip ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2017\\hmda_2017_nationwide_all-records_labels.csv\n",
      "  [2/2] Séparateur détecté: ','\n",
      "✅ 2017: 14,285,496 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2017.parquet\n"
     ]
    }
   ],
   "source": [
    "# Extraction complète des ZIP HMDA 2007–2017 puis conversion en Parquet (par année)\n",
    "# - Cherche chaque ZIP \"hmda_<year>_nationwide_all-records_labels.zip\" dans DATA_RAW\n",
    "# - Extrait le CSV interne dans un dossier temporaire\n",
    "# - Détecte le séparateur (',' ou '|')\n",
    "# - Lit en chunks et écrit un Parquet \"<DATA_WORK>/hmda_<year>.parquet\"\n",
    "\n",
    "import os, re, zipfile, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "# Dossiers\n",
    "DATA_RAW  = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\"\n",
    "DATA_WORK = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\"\n",
    "Path(DATA_WORK).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def detect_sep_from_file(txt_path: str, sample_lines: int = 20000) -> str:\n",
    "    pipe = comma = 0\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_lines: break\n",
    "            pipe  += line.count(\"|\")\n",
    "            comma += line.count(\",\")\n",
    "    return \"|\" if pipe > comma else \",\"\n",
    "\n",
    "def extract_inner_text(zip_path: str, tmp_dir: str) -> str:\n",
    "    tmp_dir = Path(tmp_dir); tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        members = [n for n in z.namelist() if n.lower().endswith((\".csv\", \".txt\", \".dat\"))]\n",
    "        if not members:\n",
    "            raise RuntimeError(f\"Aucun CSV/TXT/DAT dans {zip_path} (contenu: {z.namelist()[:5]})\")\n",
    "        member = members[0]\n",
    "        out_path = tmp_dir / Path(member).name\n",
    "        with z.open(member) as src, open(out_path, \"wb\") as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "    return str(out_path)\n",
    "\n",
    "def convert_csv_to_parquet(csv_path: str, year: int, out_parquet: str, sep: str, chunksize: int = 500_000) -> int:\n",
    "    # Supprime un ancien parquet pour éviter conflit de schéma\n",
    "    if os.path.exists(out_parquet):\n",
    "        os.remove(out_parquet)\n",
    "\n",
    "    total = 0\n",
    "    writer = None\n",
    "    schema = None\n",
    "\n",
    "    for chunk in pd.read_csv(csv_path, sep=sep, low_memory=False, chunksize=chunksize,\n",
    "                             encoding=\"utf-8\", on_bad_lines=\"skip\"):\n",
    "        if \"year\" not in chunk.columns:\n",
    "            chunk[\"year\"] = year\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "        if writer is None:\n",
    "            schema = table.schema\n",
    "            writer = pq.ParquetWriter(out_parquet, schema, use_dictionary=True)\n",
    "        writer.write_table(table)\n",
    "        total += len(chunk)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    return total\n",
    "\n",
    "# --- Traitement principal ---\n",
    "zips = [f for f in os.listdir(DATA_RAW) if f.endswith(\"_nationwide_all-records_labels.zip\")]\n",
    "zips = sorted(zips)\n",
    "\n",
    "for name in zips:\n",
    "    m = re.search(r\"(\\d{4})\", name)\n",
    "    if not m:\n",
    "        print(\"skip (pas d'année détectée):\", name)\n",
    "        continue\n",
    "    year = int(m.group(1))\n",
    "    zip_path = os.path.join(DATA_RAW, name)\n",
    "    tmp_dir  = os.path.join(Path(DATA_WORK).parent, \"_tmp_extracts\", f\"hmda_{year}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n=== {year} | Source: {name} ===\")\n",
    "        extracted = extract_inner_text(zip_path, tmp_dir)\n",
    "        print(f\"  [1/2] Extrait → {extracted}\")\n",
    "        sep = detect_sep_from_file(extracted)\n",
    "        print(f\"  [2/2] Séparateur détecté: {repr(sep)}\")\n",
    "\n",
    "        out_parquet = os.path.join(DATA_WORK, f\"hmda_{year}.parquet\")\n",
    "        n = convert_csv_to_parquet(extracted, year, out_parquet, sep=sep)\n",
    "        print(f\"✅ {year}: {n:,} lignes → {out_parquet}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ {year}: ZIP corrompu ou illisible → {zip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {year}: {e}\")\n",
    "    finally:\n",
    "        # Nettoyage du fichier extrait (garde le dossier pour debug s'il reste d'autres fichiers)\n",
    "        try:\n",
    "            if 'extracted' in locals() and os.path.exists(extracted):\n",
    "                os.remove(extracted)\n",
    "            # supprime le dossier s'il est vide\n",
    "            p = Path(tmp_dir)\n",
    "            if p.exists() and not any(p.iterdir()):\n",
    "                p.rmdir()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0809ba-e2b6-4675-ba6a-2769b4405cb2",
   "metadata": {},
   "source": [
    "**Uniquement pour les années 2010, 2013, 2014, 2015; nous avons des échecs ❌.**<br>\n",
    "**En fait nous avons des erreurs de schéma Parquet parce que certains champs numériques passent d’un chunk à l’autre de int64 à float64.<br> Comme solution simple et robuste nous allons harmoniser les types avant d’écrire**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9e606f9-fb94-47ab-a664-6f967d94cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2010 | Re-traitement (schéma compat) ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2010\\hmda_2010_nationwide_all-records_labels.csv\n",
      "  [2/2] Sep: ',' → écriture Parquet harmonisée\n",
      "✅ 2010: 16,348,557 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2010.parquet\n",
      "\n",
      "=== 2013 | Re-traitement (schéma compat) ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2013\\hmda_2013_nationwide_all-records_labels.csv\n",
      "  [2/2] Sep: ',' → écriture Parquet harmonisée\n",
      "✅ 2013: 17,016,159 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2013.parquet\n",
      "\n",
      "=== 2014 | Re-traitement (schéma compat) ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2014\\hmda_2014_nationwide_all-records_labels.csv\n",
      "  [2/2] Sep: ',' → écriture Parquet harmonisée\n",
      "✅ 2014: 12,049,341 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2014.parquet\n",
      "\n",
      "=== 2015 | Re-traitement (schéma compat) ===\n",
      "  [1/2] Extrait → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\_tmp_extracts\\hmda_2015\\hmda_2015_nationwide_all-records_labels.csv\n",
      "  [2/2] Sep: ',' → écriture Parquet harmonisée\n",
      "✅ 2015: 14,374,184 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_2015.parquet\n"
     ]
    }
   ],
   "source": [
    "# Re-traitement robuste (sans pandas.api.types.is_*): 2010, 2013, 2014, 2015\n",
    "import os, re, zipfile, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "DATA_RAW  = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\"\n",
    "DATA_WORK = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\"\n",
    "Path(DATA_WORK).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def detect_sep_from_file(txt_path: str, sample_lines: int = 20000) -> str:\n",
    "    pipe = comma = 0\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_lines: break\n",
    "            pipe  += line.count(\"|\")\n",
    "            comma += line.count(\",\")\n",
    "    return \"|\" if pipe > comma else \",\"\n",
    "\n",
    "def extract_inner_text(zip_path: str, tmp_dir: str) -> str:\n",
    "    tmp_dir = Path(tmp_dir); tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        members = [n for n in z.namelist() if n.lower().endswith((\".csv\", \".txt\", \".dat\"))]\n",
    "        if not members:\n",
    "            raise RuntimeError(f\"Aucun CSV/TXT/DAT dans {zip_path} (contenu: {z.namelist()[:5]})\")\n",
    "        member = members[0]\n",
    "        out_path = tmp_dir / Path(member).name\n",
    "        with z.open(member) as src, open(out_path, \"wb\") as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "    return str(out_path)\n",
    "\n",
    "# Colonnes textuelles connues (si présentes)\n",
    "TEXT_LIKE = {\n",
    "    \"respondent_id\", \"agency_name\", \"agency_abbr\",\n",
    "    \"loan_type_name\", \"property_type_name\", \"loan_purpose_name\",\n",
    "    \"owner_occupancy_name\", \"preapproval_name\", \"action_taken_name\",\n",
    "    \"msamd_name\", \"state_name\", \"state_abbr\", \"county_name\",\n",
    "    \"applicant_ethnicity_name\", \"co_applicant_ethnicity_name\",\n",
    "    \"applicant_race_name_1\",\"applicant_race_name_2\",\"applicant_race_name_3\",\"applicant_race_name_4\",\"applicant_race_name_5\",\n",
    "    \"co_applicant_race_name_1\",\"co_applicant_race_name_2\",\"co_applicant_race_name_3\",\"co_applicant_race_name_4\",\"co_applicant_race_name_5\",\n",
    "    \"applicant_sex_name\",\"co_applicant_sex_name\",\n",
    "    \"purchaser_type_name\",\"denial_reason_name_1\",\"denial_reason_name_2\",\"denial_reason_name_3\",\n",
    "    \"hoepa_status_name\",\"lien_status_name\",\"edit_status_name\"\n",
    "}\n",
    "\n",
    "def infer_target_types(df: pd.DataFrame, year: int) -> dict:\n",
    "    \"\"\"\n",
    "    Détermine pour chaque colonne si on la traite en 'string' (pandas StringDtype) ou 'float'\n",
    "    - colonnes connues TEXT_LIKE → string\n",
    "    - colonnes numériques natives → float\n",
    "    - colonnes object → test rapide: si >=10% des valeurs sont convertissables en nombre → float, sinon string\n",
    "    \"\"\"\n",
    "    cols = [c.strip().lower() for c in df.columns]\n",
    "    type_map = {}\n",
    "    for c in cols:\n",
    "        if c in TEXT_LIKE:\n",
    "            type_map[c] = \"string\"\n",
    "            continue\n",
    "        s = df[c]\n",
    "        # dtype-name simple pour compat\n",
    "        dt = str(s.dtype)\n",
    "        if dt.startswith((\"int\", \"float\", \"bool\")):\n",
    "            type_map[c] = \"float\"\n",
    "        elif dt == \"object\":\n",
    "            # échantillon\n",
    "            sample = s.head(5000)\n",
    "            numeric = pd.to_numeric(sample, errors=\"coerce\")\n",
    "            ratio_num = numeric.notna().mean()\n",
    "            type_map[c] = \"float\" if ratio_num >= 0.10 else \"string\"\n",
    "        else:\n",
    "            # fallback: string (catégories, etc.)\n",
    "            type_map[c] = \"string\"\n",
    "    # year forcé\n",
    "    type_map.setdefault(\"year\", \"float\")  # on laissera year en float64 pour éviter clash\n",
    "    return type_map\n",
    "\n",
    "def coerce_to_types(df: pd.DataFrame, year: int, type_map: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    if \"year\" not in df.columns:\n",
    "        df[\"year\"] = year\n",
    "    for c, t in type_map.items():\n",
    "        if c not in df.columns:\n",
    "            # colonne absente : on l'ajoute\n",
    "            df[c] = pd.Series([np.nan]*len(df))\n",
    "            if t == \"string\":\n",
    "                df[c] = df[c].astype(\"string\")\n",
    "            else:\n",
    "                df[c] = df[c].astype(\"float64\")\n",
    "            continue\n",
    "        if t == \"string\":\n",
    "            # astype string; convertit NaN proprement\n",
    "            df[c] = df[c].astype(\"string\")\n",
    "        else:\n",
    "            # float : convertir numériquement\n",
    "            if str(df[c].dtype).startswith((\"int\", \"float\", \"bool\")):\n",
    "                df[c] = df[c].astype(\"float64\")\n",
    "            else:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    # réordonner selon type_map\n",
    "    ordered_cols = list(type_map.keys())\n",
    "    # garder aussi d'éventuelles colonnes supplémentaires en fin\n",
    "    extras = [c for c in df.columns if c not in ordered_cols]\n",
    "    return df[ordered_cols + extras]\n",
    "\n",
    "def convert_csv_to_parquet_relaxed(csv_path: str, year: int, out_parquet: str, sep: str, chunksize: int = 400_000) -> int:\n",
    "    if os.path.exists(out_parquet):\n",
    "        os.remove(out_parquet)\n",
    "    total = 0\n",
    "    writer = None\n",
    "    target_schema = None\n",
    "    type_map = None\n",
    "\n",
    "    chunk_iter = pd.read_csv(csv_path, sep=sep, low_memory=False, chunksize=chunksize,\n",
    "                             encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        chunk.columns = [c.strip().lower() for c in chunk.columns]\n",
    "        # première passe: inférer la carte des types cibles sur le premier chunk\n",
    "        if type_map is None:\n",
    "            type_map = infer_target_types(chunk, year)\n",
    "        # coercition au schéma cible\n",
    "        chunk = coerce_to_types(chunk, year, type_map)\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "        if writer is None:\n",
    "            target_schema = table.schema\n",
    "            writer = pq.ParquetWriter(out_parquet, target_schema, use_dictionary=True)\n",
    "        else:\n",
    "            # alignement si colonnes manquantes\n",
    "            missing = [f.name for f in target_schema if f.name not in table.schema.names]\n",
    "            for col in missing:\n",
    "                field = target_schema.field(col)\n",
    "                if pa.types.is_string(field.type):\n",
    "                    arr = pa.array([None] * len(chunk), type=pa.string())\n",
    "                else:\n",
    "                    arr = pa.array([None] * len(chunk), type=field.type)\n",
    "                table = table.append_column(col, arr)\n",
    "            table = table.select(target_schema.names)\n",
    "        writer.write_table(table)\n",
    "        total += len(chunk)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    return total\n",
    "\n",
    "FAILED_YEARS = [2010, 2013, 2014, 2015]\n",
    "for year in FAILED_YEARS:\n",
    "    zip_name = f\"hmda_{year}_nationwide_all-records_labels.zip\"\n",
    "    zip_path = os.path.join(DATA_RAW, zip_name)\n",
    "    tmp_dir  = os.path.join(Path(DATA_WORK).parent, \"_tmp_extracts\", f\"hmda_{year}\")\n",
    "    try:\n",
    "        print(f\"\\n=== {year} | Re-traitement (schéma compat) ===\")\n",
    "        extracted = extract_inner_text(zip_path, tmp_dir)\n",
    "        print(f\"  [1/2] Extrait → {extracted}\")\n",
    "        sep = detect_sep_from_file(extracted)\n",
    "        print(f\"  [2/2] Sep: {repr(sep)} → écriture Parquet harmonisée\")\n",
    "        out_parquet = os.path.join(DATA_WORK, f\"hmda_{year}.parquet\")\n",
    "        n = convert_csv_to_parquet_relaxed(extracted, year, out_parquet, sep=sep)\n",
    "        print(f\"✅ {year}: {n:,} lignes → {out_parquet}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {year}: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            if 'extracted' in locals() and os.path.exists(extracted):\n",
    "                os.remove(extracted)\n",
    "            p = Path(tmp_dir)\n",
    "            if p.exists() and not any(p.iterdir()):\n",
    "                p.rmdir()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85a367-0743-4cef-9a16-5a4ba31a86fb",
   "metadata": {},
   "source": [
    "**Filtrer 2007–2017 (nationwide) → Tri-State (NY/NJ/CT)**<br>\n",
    "*Nous allons Créer des hmda_tristate_<year>.parquet pour 2007–2017 à partir des Parquet nationwide.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36128d2c-00d1-4e75-a886-e9761e9487ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ déjà présent: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2007.parquet\n",
      "✔️ déjà présent: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2008.parquet\n",
      "✔️ déjà présent: C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2009.parquet\n",
      "→ 2010: filtre Tri-State (streaming)\n",
      "✅ 2010: 1,234,294 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2010.parquet\n",
      "→ 2011: filtre Tri-State (streaming)\n",
      "✅ 2011: 1,132,414 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2011.parquet\n",
      "→ 2012: filtre Tri-State (streaming)\n",
      "✅ 2012: 1,322,973 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2012.parquet\n",
      "→ 2013: filtre Tri-State (streaming)\n",
      "✅ 2013: 1,186,639 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2013.parquet\n",
      "→ 2014: filtre Tri-State (streaming)\n",
      "✅ 2014: 793,587 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2014.parquet\n",
      "→ 2015: filtre Tri-State (streaming)\n",
      "✅ 2015: 926,891 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2015.parquet\n",
      "→ 2016: filtre Tri-State (streaming)\n",
      "✅ 2016: 1,023,587 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2016.parquet\n",
      "→ 2017: filtre Tri-State (streaming)\n",
      "✅ 2017: 925,866 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2017.parquet\n"
     ]
    }
   ],
   "source": [
    "# Filtrage Tri-State 2007–2017 SANS pandas (streaming PyArrow)\n",
    "# - lit chaque Parquet nationwide en mode dataset\n",
    "# - applique un filtre sur state_abbr ou state_code\n",
    "# - écrit hmda_tristate_<year>.parquet en streaming (batches)\n",
    "\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_WORK = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\"\n",
    "Path(DATA_WORK).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRI_STATES = [\"NY\",\"NJ\",\"CT\"]\n",
    "TRI_FIPS = [36, 34, 9]  # NY, NJ, CT\n",
    "\n",
    "def filter_and_write_streaming(src_parquet: str, dst_parquet: str, batch_size: int = 131072) -> int:\n",
    "    \"\"\"\n",
    "    Lit un Parquet 'src_parquet' en streaming (pyarrow.dataset), filtre Tri-State,\n",
    "    et écrit 'dst_parquet' en append. Retourne le nombre de lignes écrites.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(src_parquet, format=\"parquet\")\n",
    "    names = set(dataset.schema.names)\n",
    "\n",
    "    # Filtre selon colonnes disponibles (pré-2018 : state_abbr ou state_code)\n",
    "    if \"state_abbr\" in names:\n",
    "        filt = pc.is_in(pc.field(\"state_abbr\"), pa.array(TRI_STATES, type=pa.string()))\n",
    "    elif \"state_code\" in names:\n",
    "        # certains millésimes codent state_code en float -> cast en int64 avant comparaison\n",
    "        filt = pc.is_in(pc.cast(pc.field(\"state_code\"), pa.int64()), pa.array(TRI_FIPS, type=pa.int64()))\n",
    "    else:\n",
    "        raise RuntimeError(f\"Aucune colonne d'État détectée dans: {src_parquet}\")\n",
    "\n",
    "    # Scanner en streaming avec filtre\n",
    "    scanner = ds.Scanner.from_dataset(\n",
    "        dataset,\n",
    "        filter=filt,\n",
    "        columns=None,            # ou liste pour projeter un sous-ensemble de colonnes\n",
    "        batch_size=batch_size    # augmenter/diminuer selon RAM\n",
    "    )\n",
    "\n",
    "    total = 0\n",
    "    writer = None\n",
    "    try:\n",
    "        for batch in scanner.to_batches():\n",
    "            if batch.num_rows == 0:\n",
    "                continue\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            if writer is None:\n",
    "                # Crée le writer au premier batch, en fixant le schéma\n",
    "                writer = pq.ParquetWriter(dst_parquet, table.schema, use_dictionary=True)\n",
    "            writer.write_table(table)\n",
    "            total += table.num_rows\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    return total\n",
    "\n",
    "# Traite 2007 → 2017\n",
    "for year in range(2007, 2018):\n",
    "    src = os.path.join(DATA_WORK, f\"hmda_{year}.parquet\")            # nationwide existant\n",
    "    dst = os.path.join(DATA_WORK, f\"hmda_tristate_{year}.parquet\")   # sortie Tri-State\n",
    "    if not os.path.exists(src):\n",
    "        print(f\"skip {year}: introuvable → {src}\")\n",
    "        continue\n",
    "    if os.path.exists(dst):\n",
    "        print(f\"✔️ déjà présent: {dst}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"→ {year}: filtre Tri-State (streaming)\")\n",
    "    rows = filter_and_write_streaming(src, dst, batch_size=131072)\n",
    "    print(f\"✅ {year}: {rows:,} lignes → {dst}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098acf1-74e7-46bf-b4e4-0cd535a47414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724184d-7880-4d99-a1a8-38c890753a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059580c-882b-41b8-84be-b31ba1dbf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e1b06-813a-4f86-8448-5f5701137e56",
   "metadata": {},
   "source": [
    "**5 HMDA Tri-State (NY, NJ, CT) — 2018→2024 via FFIEC Data Browser API**<br>\n",
    "        *le but ici est de profiter de l'existence de l'API qui est une garantie de l'accessibilité et la fiabilité de la data*<br>\n",
    "        *Malheureusement elle à ne possède pas de données ultérieure à 2018 d'où l'approche par teléchargement manuel ci dessus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78152560-37a0-4db0-815e-f1c5c402f6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2018 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2018.csv.gz\n",
      "=== 2018 — conversion Parquet ===\n",
      "❌ 2018: Not a gzipped file (b'ac')\n",
      "\n",
      "=== 2019 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2019.csv.gz\n",
      "=== 2019 — conversion Parquet ===\n",
      "✅ 2019: 1,181,766 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2019.parquet\n",
      "\n",
      "=== 2020 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2020.csv.gz\n",
      "=== 2020 — conversion Parquet ===\n",
      "✅ 2020: 1,632,042 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2020.parquet\n",
      "\n",
      "=== 2021 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2021.csv.gz\n",
      "=== 2021 — conversion Parquet ===\n",
      "✅ 2021: 1,759,722 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2021.parquet\n",
      "\n",
      "=== 2022 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2022.csv.gz\n",
      "=== 2022 — conversion Parquet ===\n",
      "✅ 2022: 1,126,036 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2022.parquet\n",
      "\n",
      "=== 2023 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2023.csv.gz\n",
      "=== 2023 — conversion Parquet ===\n",
      "✅ 2023: 801,200 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2023.parquet\n",
      "\n",
      "=== 2024 — téléchargement Tri-State (API CSV/gzip) ===\n",
      "   CSV.gz → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\\hmda_tristate_2024.csv.gz\n",
      "=== 2024 — conversion Parquet ===\n",
      "✅ 2024: 828,720 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2024.parquet\n",
      "\n",
      "=== RÉSUMÉ ===\n",
      "2018: fail\n",
      "2019: ok (1,181,766 rows)\n",
      "2020: ok (1,632,042 rows)\n",
      "2021: ok (1,759,722 rows)\n",
      "2022: ok (1,126,036 rows)\n",
      "2023: ok (801,200 rows)\n",
      "2024: ok (828,720 rows)\n"
     ]
    }
   ],
   "source": [
    "# HMDA Tri-State (NY, NJ, CT) — 2018→2024 via FFIEC Data Browser API\n",
    "# Corrigé : accepte les réponses gzip (CSV compressé) et convertit en Parquet.\n",
    "\n",
    "import os, time, gzip, io, requests, pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "# Répertoires\n",
    "DATA_RAW  = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\"\n",
    "DATA_WORK = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\"\n",
    "Path(DATA_RAW).mkdir(parents=True, exist_ok=True)\n",
    "Path(DATA_WORK).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "YEARS       = list(range(2018, 2025))\n",
    "TRI_STATES  = [\"NY\",\"NJ\",\"CT\"]\n",
    "ACTIONS_ALL = \"1,2,3,4,5,6,7,8\"   # filtre HMDA requis\n",
    "CSV_ENDPOINT = \"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv\"\n",
    "\n",
    "def _is_gzip_header(b: bytes) -> bool:\n",
    "    return len(b) >= 2 and b[0] == 0x1F and b[1] == 0x8B\n",
    "\n",
    "def download_csv_tristate(year: int) -> str:\n",
    "    \"\"\"\n",
    "    Télécharge le CSV Tri-State. Si l'API renvoie du gzip (Content-Encoding: gzip ou header 1F 8B),\n",
    "    on sauvegarde tel quel en .csv.gz ; sinon on compresse à la volée.\n",
    "    \"\"\"\n",
    "    params  = {\"years\": str(year), \"states\": \",\".join(TRI_STATES), \"actions_taken\": ACTIONS_ALL}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept-Encoding\": \"gzip\"}  # laisser le serveur gziper\n",
    "    out_gz  = os.path.join(DATA_RAW, f\"hmda_tristate_{year}.csv.gz\")\n",
    "    tmp_gz  = out_gz + \".part\"\n",
    "    for p in (tmp_gz,):\n",
    "        if os.path.exists(p):\n",
    "            try: os.remove(p)\n",
    "            except: pass\n",
    "\n",
    "    with requests.get(CSV_ENDPOINT, params=params, stream=True, timeout=1200, headers=headers) as r:\n",
    "        r.raise_for_status()\n",
    "\n",
    "        # On lit un premier chunk pour déterminer si la réponse est déjà gzippée\n",
    "        it = r.iter_content(chunk_size=1<<20)\n",
    "        first = b\"\"\n",
    "        for chunk in it:\n",
    "            if chunk:\n",
    "                first = chunk\n",
    "                break\n",
    "        if not first:\n",
    "            raise RuntimeError(f\"{year}: Réponse vide\")\n",
    "\n",
    "        # Si la réponse est déjà gzip (soit via Content-Encoding, soit via header magique), on écrit tel quel\n",
    "        already_gzip = (\"gzip\" in r.headers.get(\"Content-Encoding\",\"\").lower()) or _is_gzip_header(first)\n",
    "\n",
    "        if already_gzip:\n",
    "            with open(tmp_gz, \"wb\") as f:\n",
    "                f.write(first)\n",
    "                for chunk in it:\n",
    "                    if chunk: f.write(chunk)\n",
    "            os.replace(tmp_gz, out_gz)\n",
    "            return out_gz\n",
    "        else:\n",
    "            # Réponse texte CSV non compressée → on compresse à la volée en .csv.gz\n",
    "            with gzip.open(tmp_gz, \"wb\") as f:\n",
    "                f.write(first)\n",
    "                for chunk in it:\n",
    "                    if chunk: f.write(chunk)\n",
    "            os.replace(tmp_gz, out_gz)\n",
    "            return out_gz\n",
    "\n",
    "def csv_gz_to_parquet(csv_gz_path: str, year: int, out_parquet: str, chunksize: int = 250_000) -> int:\n",
    "    if os.path.exists(out_parquet):\n",
    "        os.remove(out_parquet)\n",
    "    total = 0\n",
    "    for chunk in pd.read_csv(csv_gz_path, compression=\"gzip\", low_memory=False, chunksize=chunksize):\n",
    "        if \"year\" not in chunk.columns:\n",
    "            chunk[\"year\"] = year\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "        if not os.path.exists(out_parquet):\n",
    "            pq.write_table(table, out_parquet)\n",
    "        else:\n",
    "            with pq.ParquetWriter(out_parquet, table.schema, use_dictionary=True) as w:\n",
    "                w.write_table(table)\n",
    "        total += len(chunk)\n",
    "    return total\n",
    "\n",
    "report = []\n",
    "for y in YEARS:\n",
    "    try:\n",
    "        print(f\"\\n=== {y} — téléchargement Tri-State (API CSV/gzip) ===\")\n",
    "        csv_gz = download_csv_tristate(y)\n",
    "        print(f\"   CSV.gz → {csv_gz}\")\n",
    "        out_parquet = os.path.join(DATA_WORK, f\"hmda_tristate_{y}.parquet\")\n",
    "        print(f\"=== {y} — conversion Parquet ===\")\n",
    "        n = csv_gz_to_parquet(csv_gz, y, out_parquet)\n",
    "        print(f\"✅ {y}: {n:,} lignes → {out_parquet}\")\n",
    "        report.append((y, \"ok\", n))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {y}: {e}\")\n",
    "        report.append((y, \"fail\", 0))\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"\\n=== RÉSUMÉ ===\")\n",
    "for y, status, n in report:\n",
    "    print(f\"{y}: {status} ({n:,} rows)\" if status==\"ok\" else f\"{y}: {status}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68385c19-606f-4ded-9c86-ed9c90233876",
   "metadata": {},
   "source": [
    "**Seule 2018 a fait un echec ❌. En fait, il a été sauvegardé comme .csv.gz alors que le flux n’était pas gzippé.**<br> \n",
    "**Voici un correctif minimal :**<br> \n",
    "    *on ré-télécharge 2018 en forçant Accept-Encoding: identity, on compresse nous-mêmes en gzip si besoin, puis on reconvertit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d86deb9f-9814-4de9-b642-6b7f9d9a078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↻ Téléchargement robuste 2018…\n",
      "→ Conversion Parquet 2018…\n",
      "✅ 2018: 1,071,054 lignes → C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\\hmda_tristate_2018.parquet\n"
     ]
    }
   ],
   "source": [
    "# Correctif 2018 — retente avec headers alternatifs, puis fallback par État (NY, NJ, CT) + fusion\n",
    "import os, io, gzip, time, requests, pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "DATA_RAW  = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_raw\\hmda\"\n",
    "DATA_WORK = r\"C:\\Users\\33669\\OneDrive\\Документы\\data_credit_scoring\\tri_state_ai\\data_work\"\n",
    "os.makedirs(DATA_RAW, exist_ok=True)\n",
    "os.makedirs(DATA_WORK, exist_ok=True)\n",
    "\n",
    "CSV_ENDPOINT = \"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv\"\n",
    "YEAR = 2018\n",
    "TRI_STATES = [\"NY\",\"NJ\",\"CT\"]\n",
    "ACTIONS_ALL = \"1,2,3,4,5,6,7,8\"\n",
    "\n",
    "def _is_gzip_header(b: bytes) -> bool:\n",
    "    return len(b) >= 2 and b[0] == 0x1F and b[1] == 0x8B\n",
    "\n",
    "def _write_response_to_gz(resp: requests.Response, out_gz_path: str) -> None:\n",
    "    \"\"\"Écrit la réponse API dans un .csv.gz, qu'elle soit déjà gzippée ou non.\"\"\"\n",
    "    tmp = out_gz_path + \".part\"\n",
    "    if os.path.exists(tmp):\n",
    "        try: os.remove(tmp)\n",
    "        except: pass\n",
    "\n",
    "    it = resp.iter_content(chunk_size=1<<20)\n",
    "    first = b\"\"\n",
    "    for chunk in it:\n",
    "        if chunk:\n",
    "            first = chunk\n",
    "            break\n",
    "    if not first:\n",
    "        raise RuntimeError(\"Réponse vide\")\n",
    "\n",
    "    if (\"gzip\" in resp.headers.get(\"Content-Encoding\",\"\").lower()) or _is_gzip_header(first):\n",
    "        # Déjà gzippé : écrire tel quel\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            f.write(first)\n",
    "            for chunk in it:\n",
    "                if chunk: f.write(chunk)\n",
    "    else:\n",
    "        # Pas gzip : compresser à la volée\n",
    "        with gzip.open(tmp, \"wb\") as f:\n",
    "            f.write(first)\n",
    "            for chunk in it:\n",
    "                if chunk: f.write(chunk)\n",
    "\n",
    "    os.replace(tmp, out_gz_path)\n",
    "\n",
    "def _attempt_download(params: dict, header_profile: int, timeout=1200) -> requests.Response | None:\n",
    "    \"\"\"Tente un GET avec un profil d’en-têtes donné. Retourne la Response ou None si status != 200.\"\"\"\n",
    "    headers_profiles = [\n",
    "        # 0) Profil \"normal\" (gzip autorisé)\n",
    "        {\"User-Agent\":\"Mozilla/5.0\", \"Accept\":\"text/csv,*/*;q=0.8\", \"Accept-Encoding\":\"gzip\", \"Referer\":\"https://ffiec.cfpb.gov/data-browser/\", \"Origin\":\"https://ffiec.cfpb.gov\"},\n",
    "        # 1) Forcer pas de compression côté serveur\n",
    "        {\"User-Agent\":\"Mozilla/5.0\", \"Accept\":\"text/csv,*/*;q=0.8\", \"Accept-Encoding\":\"identity\", \"Referer\":\"https://ffiec.cfpb.gov/data-browser/\", \"Origin\":\"https://ffiec.cfpb.gov\"},\n",
    "        # 2) UA différent\n",
    "        {\"User-Agent\":\"curl/8.0\", \"Accept\":\"text/csv,*/*;q=0.8\", \"Accept-Encoding\":\"gzip\", \"Referer\":\"https://ffiec.cfpb.gov/data-browser/\", \"Origin\":\"https://ffiec.cfpb.gov\"},\n",
    "    ]\n",
    "    headers = headers_profiles[header_profile % len(headers_profiles)]\n",
    "    r = requests.get(CSV_ENDPOINT, params=params, stream=True, timeout=timeout, headers=headers, allow_redirects=True)\n",
    "    if r.status_code == 200:\n",
    "        ctype = r.headers.get(\"Content-Type\",\"\").lower()\n",
    "        # L'API livre parfois gzip + text/csv; parfois application/octet-stream : on accepte si 200\n",
    "        if \"text/csv\" in ctype or \"octet-stream\" in ctype or \"csv\" in ctype:\n",
    "            return r\n",
    "        # Si 200 mais content-type inattendu, on tente quand même (on inspectera l’en-tête gzip)\n",
    "        return r\n",
    "    return None\n",
    "\n",
    "def download_2018_tristate_robust(out_gz_path: str) -> None:\n",
    "    # 1) Essai direct Tri-State (toutes les têtes, avec backoff)\n",
    "    params = {\"years\": str(YEAR), \"states\": \",\".join(TRI_STATES), \"actions_taken\": ACTIONS_ALL}\n",
    "    for attempt in range(6):\n",
    "        resp = _attempt_download(params, header_profile=attempt)\n",
    "        if resp is not None:\n",
    "            _write_response_to_gz(resp, out_gz_path)\n",
    "            return\n",
    "        # backoff progressif\n",
    "        time.sleep(1 + attempt)\n",
    "\n",
    "    # 2) Fallback: télécharger par État et fusionner\n",
    "    tmp_parts = []\n",
    "    try:\n",
    "        for st in TRI_STATES:\n",
    "            p = {\"years\": str(YEAR), \"states\": st, \"actions_taken\": ACTIONS_ALL}\n",
    "            part_gz = os.path.join(DATA_RAW, f\"hmda_{YEAR}_{st}.csv.gz\")\n",
    "            # plusieurs tentatives d’en-têtes pour cet État\n",
    "            ok = False\n",
    "            for attempt in range(6):\n",
    "                resp = _attempt_download(p, header_profile=attempt)\n",
    "                if resp is not None:\n",
    "                    _write_response_to_gz(resp, part_gz)\n",
    "                    ok = True\n",
    "                    break\n",
    "                time.sleep(1 + attempt)\n",
    "            if not ok:\n",
    "                raise RuntimeError(f\"Impossible de télécharger {YEAR} pour l'État {st}\")\n",
    "            tmp_parts.append(part_gz)\n",
    "\n",
    "        # 3) Fusion des 3 fichiers .csv.gz en un seul .csv.gz\n",
    "        #    On décompresse chaque part puis on recompresse dans out_gz_path, en gardant l'entête une seule fois.\n",
    "        tmp = out_gz_path + \".part\"\n",
    "        if os.path.exists(tmp):\n",
    "            try: os.remove(tmp)\n",
    "            except: pass\n",
    "\n",
    "        header_written = False\n",
    "        with gzip.open(tmp, \"wb\") as fout:\n",
    "            for i, part in enumerate(tmp_parts):\n",
    "                with gzip.open(part, \"rb\") as fin:\n",
    "                    for j, line in enumerate(fin):\n",
    "                        if not header_written:\n",
    "                            fout.write(line)\n",
    "                            header_written = True\n",
    "                        else:\n",
    "                            # sauter l'entête des fichiers suivants\n",
    "                            if j == 0:\n",
    "                                continue\n",
    "                            fout.write(line)\n",
    "        os.replace(tmp, out_gz_path)\n",
    "    finally:\n",
    "        # Nettoyage des parts\n",
    "        for pth in tmp_parts:\n",
    "            try: os.remove(pth)\n",
    "            except: pass\n",
    "\n",
    "def csv_gz_to_parquet(csv_gz_path: str, year: int, out_parquet: str, chunksize: int = 250_000) -> int:\n",
    "    if os.path.exists(out_parquet):\n",
    "        os.remove(out_parquet)\n",
    "    total = 0\n",
    "    for chunk in pd.read_csv(csv_gz_path, compression=\"gzip\", low_memory=False, chunksize=chunksize):\n",
    "        if \"year\" not in chunk.columns:\n",
    "            chunk[\"year\"] = year\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "        if not os.path.exists(out_parquet):\n",
    "            pq.write_table(table, out_parquet)\n",
    "        else:\n",
    "            with pq.ParquetWriter(out_parquet, table.schema, use_dictionary=True) as w:\n",
    "                w.write_table(table)\n",
    "        total += len(chunk)\n",
    "    return total\n",
    "\n",
    "# --- Lance la correction 2018 ---\n",
    "out_csv_gz = os.path.join(DATA_RAW, f\"hmda_tristate_{YEAR}.csv.gz\")\n",
    "print(\"↻ Téléchargement robuste 2018…\")\n",
    "download_2018_tristate_robust(out_csv_gz)\n",
    "\n",
    "out_parquet = os.path.join(DATA_WORK, f\"hmda_tristate_{YEAR}.parquet\")\n",
    "print(\"→ Conversion Parquet 2018…\")\n",
    "n = csv_gz_to_parquet(out_csv_gz, YEAR, out_parquet)\n",
    "print(f\"✅ 2018: {n:,} lignes → {out_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252aa04-1d2e-4800-a442-55552f527b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c87c605d-2a53-4f94-a767-982dc8887454",
   "metadata": {},
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"*MISSION 1 COMPLETE*\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
